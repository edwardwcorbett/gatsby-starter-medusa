"use strict";

var _interopRequireDefault = require("@babel/runtime/helpers/interopRequireDefault");

exports.__esModule = true;
exports.fetchRemoteFile = fetchRemoteFile;

var _got = _interopRequireDefault(require("got"));

var _fileType = _interopRequireDefault(require("file-type"));

var _path = _interopRequireDefault(require("path"));

var _fsExtra = _interopRequireDefault(require("fs-extra"));

var _createContentDigest = require("./create-content-digest");

var _filenameUtils = require("./filename-utils");

// copied from gatsby-worker
const IS_WORKER = !!(process.send && process.env.GATSBY_WORKER_MODULE_PATH);
const WORKER_ID = process.env.GATSBY_WORKER_ID;

const cacheIdForWorkers = url => `remote-file-workers-${url}`;

const cacheIdForHeaders = url => `remote-file-headers-${url}`;

const cacheIdForExtensions = url => `remote-file-extension-${url}`;

const STALL_RETRY_LIMIT = process.env.GATSBY_STALL_RETRY_LIMIT ? parseInt(process.env.GATSBY_STALL_RETRY_LIMIT, 10) : 3;
const STALL_TIMEOUT = process.env.GATSBY_STALL_TIMEOUT ? parseInt(process.env.GATSBY_STALL_TIMEOUT, 10) : 30000;
const CONNECTION_TIMEOUT = process.env.GATSBY_CONNECTION_TIMEOUT ? parseInt(process.env.GATSBY_CONNECTION_TIMEOUT, 10) : 30000;
const INCOMPLETE_RETRY_LIMIT = process.env.GATSBY_INCOMPLETE_RETRY_LIMIT ? parseInt(process.env.GATSBY_INCOMPLETE_RETRY_LIMIT, 10) : 3;
let fetchCache = new Map();
let latestBuildId = ``;

async function fetchRemoteFile(args) {
  var _global$__GATSBY$buil, _global$__GATSBY;

  const BUILD_ID = (_global$__GATSBY$buil = (_global$__GATSBY = global.__GATSBY) === null || _global$__GATSBY === void 0 ? void 0 : _global$__GATSBY.buildId) !== null && _global$__GATSBY$buil !== void 0 ? _global$__GATSBY$buil : ``;

  if (BUILD_ID !== latestBuildId) {
    latestBuildId = BUILD_ID;
    fetchCache = new Map();
  } // If we are already fetching the file, return the unresolved promise


  const inFlight = fetchCache.get(args.url);

  if (inFlight) {
    return inFlight;
  } // Create file fetch promise and store it into cache


  const fetchPromise = fetchFile(args);
  fetchCache.set(args.url, fetchPromise);
  return fetchPromise.catch(err => {
    fetchCache.delete(args.url);
    throw err;
  });
}

function pollUntilComplete(cache, url, buildId, cb) {
  if (!IS_WORKER) {
    // We are not in a worker, so we shouldn't use the cache
    return void cb();
  }

  cache.get(cacheIdForWorkers(url)).then(entry => {
    if (!entry || entry.buildId !== buildId) {
      return void cb();
    }

    if (entry.status === `complete`) {
      cb(undefined, entry.result);
    } else if (entry.status === `failed`) {
      cb(new Error(entry.result));
    } else {
      setTimeout(() => {
        pollUntilComplete(cache, url, buildId, cb); // Magic number
      }, 500);
    }

    return undefined;
  });
  return undefined;
} // TODO Add proper mutex instead of file cache hacks


async function fetchFile({
  url,
  cache,
  auth = {},
  httpHeaders = {},
  ext,
  name
}) {
  var _global$__GATSBY$buil2, _global$__GATSBY2;

  // global introduced in gatsby 4.0.0
  const BUILD_ID = (_global$__GATSBY$buil2 = (_global$__GATSBY2 = global.__GATSBY) === null || _global$__GATSBY2 === void 0 ? void 0 : _global$__GATSBY2.buildId) !== null && _global$__GATSBY$buil2 !== void 0 ? _global$__GATSBY$buil2 : ``;
  const pluginCacheDir = cache.directory; // when a cache entry is present we wait until it completes

  const result = await new Promise((resolve, reject) => {
    pollUntilComplete(cache, url, BUILD_ID, (err, result) => {
      if (err) {
        return reject(err);
      }

      return resolve(result);
    });
  });

  if (result) {
    return result;
  }

  if (IS_WORKER) {
    await cache.set(cacheIdForWorkers(url), {
      status: `pending`,
      result: null,
      workerId: WORKER_ID,
      buildId: BUILD_ID
    });
  } // See if there's response headers for this url
  // from a previous request.


  const cachedHeaders = await cache.get(cacheIdForHeaders(url));
  const headers = { ...httpHeaders
  };

  if (cachedHeaders && cachedHeaders.etag) {
    headers[`If-None-Match`] = cachedHeaders.etag;
  } // Add htaccess authentication if passed in. This isn't particularly
  // extensible. We should define a proper API that we validate.


  const httpOptions = {};

  if (auth && (auth.htaccess_pass || auth.htaccess_user)) {
    httpOptions.username = auth.htaccess_user;
    httpOptions.password = auth.htaccess_pass;
  } // Create the temp and permanent file names for the url.


  let digest = (0, _createContentDigest.createContentDigest)(url); // if worker id is present - we also append the worker id until we have a proper mutex

  if (IS_WORKER) {
    digest += `-${WORKER_ID}`;
  }

  if (!name) {
    name = (0, _filenameUtils.getRemoteFileName)(url);
  }

  if (!ext) {
    ext = (0, _filenameUtils.getRemoteFileExtension)(url);
  }

  const tmpFilename = (0, _filenameUtils.createFilePath)(pluginCacheDir, `tmp-${digest}`, ext); // Fetch the file.

  try {
    const response = await requestRemoteNode(url, headers, tmpFilename, httpOptions);

    if (response.statusCode === 200) {
      // Save the response headers for future requests.
      await cache.set(cacheIdForHeaders(url), response.headers); // If the user did not provide an extension and we couldn't get one from remote file, try and guess one

      if (!ext) {
        // if this is fresh response - try to guess extension and cache result for future
        const filetype = await _fileType.default.fromFile(tmpFilename);

        if (filetype) {
          ext = `.${filetype.ext}`;
          await cache.set(cacheIdForExtensions(url), ext);
        }
      }
    } else if (response.statusCode === 304) {
      if (!ext) {
        ext = await cache.get(cacheIdForExtensions(url));
      }
    } // Multiple workers have started the fetch and we need another check to only let one complete


    const cacheEntry = await cache.get(cacheIdForWorkers(url));

    if (cacheEntry && cacheEntry.workerId !== WORKER_ID) {
      return new Promise((resolve, reject) => {
        pollUntilComplete(cache, url, BUILD_ID, (err, result) => {
          if (err) {
            return reject(err);
          }

          return resolve(result);
        });
      });
    } // If the status code is 200, move the piped temp file to the real name.


    const filename = (0, _filenameUtils.createFilePath)(_path.default.join(pluginCacheDir, digest), name, ext);

    if (response.statusCode === 200) {
      await _fsExtra.default.move(tmpFilename, filename, {
        overwrite: true
      }); // Else if 304, remove the empty response.
    } else {
      await _fsExtra.default.remove(tmpFilename);
    }

    if (IS_WORKER) {
      await cache.set(cacheIdForWorkers(url), {
        status: `complete`,
        result: filename,
        workerId: WORKER_ID,
        buildId: BUILD_ID
      });
    }

    return filename;
  } catch (err) {
    // enable multiple workers to continue when done
    if (IS_WORKER) {
      const cacheEntry = await cache.get(cacheIdForWorkers(url));

      if (!cacheEntry || cacheEntry.workerId === WORKER_ID) {
        await cache.set(cacheIdForWorkers(url), {
          status: `failed`,
          result: err.toString ? err.toString() : err.message ? err.message : err,
          workerId: WORKER_ID,
          buildId: BUILD_ID
        });
      }
    }

    throw err;
  }
}
/**
 * requestRemoteNode
 * --
 * Download the requested file
 *
 * @param  {String}   url
 * @param  {Headers}  headers
 * @param  {String}   tmpFilename
 * @param  {Object}   httpOptions
 * @param  {number}   attempt
 * @return {Promise<Object>}  Resolves with the [http Result Object]{@link https://nodejs.org/api/http.html#http_class_http_serverresponse}
 */


function requestRemoteNode(url, headers, tmpFilename, httpOptions, attempt = 1) {
  return new Promise((resolve, reject) => {
    let timeout;

    const fsWriteStream = _fsExtra.default.createWriteStream(tmpFilename);

    fsWriteStream.on(`error`, error => {
      if (timeout) {
        clearTimeout(timeout);
      }

      reject(error);
    }); // Called if we stall for 30s without receiving any data

    const handleTimeout = async () => {
      fsWriteStream.close();

      _fsExtra.default.removeSync(tmpFilename);

      if (attempt < STALL_RETRY_LIMIT) {
        // Retry by calling ourself recursively
        resolve(requestRemoteNode(url, headers, tmpFilename, httpOptions, attempt + 1));
      } else {
        // TODO move to new Error type
        // eslint-disable-next-line prefer-promise-reject-errors
        reject(`Failed to download ${url} after ${STALL_RETRY_LIMIT} attempts`);
      }
    };

    const resetTimeout = () => {
      if (timeout) {
        clearTimeout(timeout);
      }

      timeout = setTimeout(handleTimeout, STALL_TIMEOUT);
    };

    const responseStream = _got.default.stream(url, {
      headers,
      timeout: {
        send: CONNECTION_TIMEOUT // https://github.com/sindresorhus/got#timeout

      },
      ...httpOptions,
      isStream: true
    });

    let haveAllBytesBeenWritten = false; // Fixes a bug in latest got where progress.total gets reset when stream ends, even if it wasn't complete.

    let totalSize = null;
    responseStream.on(`downloadProgress`, progress => {
      if (progress.total != null && (!totalSize || totalSize < progress.total)) {
        totalSize = progress.total;
      }

      if (progress.transferred === totalSize || totalSize === null) {
        haveAllBytesBeenWritten = true;
      }
    });
    responseStream.pipe(fsWriteStream); // If there's a 400/500 response or other error.
    // it will trigger a finish event on fsWriteStream

    responseStream.on(`error`, error => {
      if (timeout) {
        clearTimeout(timeout);
      }

      fsWriteStream.close();

      _fsExtra.default.removeSync(tmpFilename);

      process.nextTick(() => {
        reject(error);
      });
    });
    responseStream.on(`response`, response => {
      resetTimeout();
      fsWriteStream.once(`finish`, () => {
        if (timeout) {
          clearTimeout(timeout);
        } // We have an incomplete download


        if (!haveAllBytesBeenWritten) {
          _fsExtra.default.removeSync(tmpFilename);

          if (attempt < INCOMPLETE_RETRY_LIMIT) {
            // let's give node time to remove the file
            process.nextTick(() => resolve(requestRemoteNode(url, headers, tmpFilename, httpOptions, attempt + 1)));
            return undefined;
          } else {
            // TODO move to new Error type
            // eslint-disable-next-line prefer-promise-reject-errors
            return reject(`Failed to download ${url} after ${INCOMPLETE_RETRY_LIMIT} attempts`);
          }
        }

        return resolve(response);
      });
    });
  });
}